{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7de0c225",
   "metadata": {},
   "source": [
    "## Update Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da3c9fa",
   "metadata": {},
   "source": [
    "### Feature Lifecycle\n",
    "Feature paths:\n",
    "1. Lost early → MSCKF (nullspace)\n",
    "2. At marg time, short track → MSCKF (nullspace)\n",
    "3. At marg time, max track, SLAM full → MSCKF (nullspace)\n",
    "4. At marg time, max track, SLAM available → SLAM (added to state)\n",
    "5. Already SLAM → SLAM (update existing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb969559",
   "metadata": {},
   "source": [
    "                          Feature at time t\n",
    "                                 │\n",
    "                    ┌────────────┴────────────┐\n",
    "                    │                         │\n",
    "              Lost tracking?           Still tracked?\n",
    "                    │                         │\n",
    "                    ▼                         ▼\n",
    "         ┌──────────────────┐      ┌──────────────────┐\n",
    "         │  feats_lost      │      │ At oldest clone? │\n",
    "         │  → MSCKF         │      └──────────────────┘\n",
    "         └──────────────────┘               │\n",
    "                                    ┌───────┴───────┐\n",
    "                                   YES              NO\n",
    "                                    │               │\n",
    "                                    ▼               ▼\n",
    "                         ┌──────────────────┐  Continue tracking\n",
    "                         │  feats_marg      │  (not used yet)\n",
    "                         └──────────────────┘\n",
    "                                    │\n",
    "                         Track length > max_clone_size?\n",
    "                                    │\n",
    "                         ┌──────────┴──────────┐\n",
    "                        YES                    NO\n",
    "                         │                     │\n",
    "                         ▼                     ▼\n",
    "              ┌──────────────────┐   ┌──────────────────┐\n",
    "              │ feats_maxtracks  │   │  feats_marg      │\n",
    "              └──────────────────┘   │  → MSCKF         │\n",
    "                         │           └──────────────────┘\n",
    "              SLAM budget available?\n",
    "                         │\n",
    "              ┌──────────┴──────────┐\n",
    "             YES                    NO\n",
    "              │                     │\n",
    "              ▼                     ▼\n",
    "    ┌──────────────────┐  ┌──────────────────┐\n",
    "    │  → SLAM          │  │  → MSCKF         │\n",
    "    │  (delayed_init)  │  │  (nullspace)     │\n",
    "    └──────────────────┘  └──────────────────┘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9545e7cb",
   "metadata": {},
   "source": [
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                          FEATURE TRACKING PHASE                             │\n",
    "│                     (Features tracked across frames)                        │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                      │\n",
    "                                      ▼\n",
    "                    ┌─────────────────────────────────┐\n",
    "                    │   New Image at time t arrives   │\n",
    "                    └─────────────────────────────────┘\n",
    "                                      │\n",
    "                                      ▼\n",
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
    "┃                         FEATURE CATEGORIZATION                            ┃\n",
    "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
    "                                      │\n",
    "                ┌─────────────────────┼─────────────────────┐\n",
    "                ▼                     ▼                     ▼\n",
    "    ┌───────────────────┐ ┌───────────────────┐ ┌──────────────────────┐\n",
    "    │  Lost Features    │ │ Features at Oldest│ │ Already SLAM Features│\n",
    "    │   (feats_lost)    │ │Clone (feats_marg) │ │   (continue update)  │\n",
    "    │                   │ │                   │ │                      │\n",
    "    │ NOT in newest     │ │ Present at time   │ │ Pull from trackFEATS │\n",
    "    │ frame anymore     │ │ margtimestep()    │ │ or trackARUCO        │\n",
    "    └───────────────────┘ └───────────────────┘ └──────────────────────┘\n",
    "            │                       │                        │\n",
    "            │                       ▼                        │\n",
    "            │          ┌──────────────────────┐             │\n",
    "            │          │  Check Track Length  │             │\n",
    "            │          │ > max_clone_size?    │             │\n",
    "            │          └──────────────────────┘             │\n",
    "            │                 │         │                    │\n",
    "            │                 ▼         ▼                    │\n",
    "            │          ┌─────YES────┐  NO                   │\n",
    "            │          │            │  │                     │\n",
    "            │          ▼            │  │                     │\n",
    "            │  ┌──────────────────┐│  │                     │\n",
    "            │  │ feats_maxtracks  ││  │                     │\n",
    "            │  │                  ││  │                     │\n",
    "            │  │ Long-lived       ││  │                     │\n",
    "            │  │ features eligible││  │                     │\n",
    "            │  │ for SLAM         ││  │                     │\n",
    "            │  └──────────────────┘│  │                     │\n",
    "            │          │            │  │                     │\n",
    "            │          ▼            ▼  ▼                     │\n",
    "            │  ┌──────────────────────────────┐             │\n",
    "            │  │  SLAM Budget Available?      │             │\n",
    "            │  │  - max_slam_features > 0?    │             │\n",
    "            │  │  - dt_slam_delay passed?     │             │\n",
    "            │  │  - slots available?          │             │\n",
    "            │  └──────────────────────────────┘             │\n",
    "            │          │                │                    │\n",
    "            │    ┌─────YES───────┐   NO│                    │\n",
    "            │    ▼                │     │                    │\n",
    "            │ ┌──────────────┐   │     │                    │\n",
    "            │ │ feats_slam   │   │     │                    │\n",
    "            │ │ (DELAYED)    │   │     │                    │\n",
    "            │ └──────────────┘   │     │                    │\n",
    "            │    │                │     │                    │\n",
    "            └────┼────────────────┼─────┼────────────────────┘\n",
    "                 │                │     │\n",
    "                 │                └─────┴───────────┐\n",
    "                 │                                  │\n",
    "                 │                                  ▼\n",
    "                 │                    ┌──────────────────────────────┐\n",
    "                 │                    │    featsup_MSCKF             │\n",
    "                 │                    │                              │\n",
    "                 │                    │ = feats_lost                 │\n",
    "                 │                    │ + feats_marg (not maxtrack)  │\n",
    "                 │                    │ + feats_maxtracks (rejected) │\n",
    "                 │                    └──────────────────────────────┘\n",
    "                 │                                  │\n",
    "┏━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━┓\n",
    "┃                ▼                                 ▼                   ┃\n",
    "┃   ┌─────────────────────────┐      ┌─────────────────────────────┐ ┃\n",
    "┃   │  UpdaterSLAM Pipeline   │      │   UpdaterMSCKF Pipeline     │ ┃\n",
    "┃   └─────────────────────────┘      └─────────────────────────────┘ ┃\n",
    "┃                │                                 │                   ┃\n",
    "┃                ▼                                 ▼                   ┃\n",
    "┃   ┌─────────────────────────┐      ┌─────────────────────────────┐ ┃\n",
    "┃   │ 1. delayed_init()       │      │ 1. Triangulate features     │ ┃\n",
    "┃   │    - Triangulate        │      │                             │ ┃\n",
    "┃   │    - Chi² test          │      │ 2. Compute Jacobians:       │ ┃\n",
    "┃   │    - Initialize to      │      │    • H_f (w.r.t. feature)   │ ┃\n",
    "┃   │      state vector       │      │    • H_x (w.r.t. state)     │ ┃\n",
    "┃   │    - Expand covariance  │      │                             │ ┃\n",
    "┃   │                         │      │ 3. Nullspace Projection     │ ┃\n",
    "┃   │ 2. update()             │      │                             │ ┃\n",
    "┃   │    - Update existing    │      │                             | ┃\n",
    "┃   │      SLAM features      │      │                             │ ┃\n",
    "┃   │    - Chi² test          │      │                             │ ┃\n",
    "┃   │    - H_xf includes      │      │ 4. Chi² test                │ ┃\n",
    "┃   │      feature Jacobian   │      │                             │ ┃\n",
    "┃   │    - EKF update         │      │ 5. EKF update (state only)  │ ┃\n",
    "┃   └─────────────────────────┘      │                             │ ┃\n",
    "┃                │                    └─────────────────────────────┘ ┃\n",
    "┃                │                                 │                   ┃\n",
    "┗━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━┛\n",
    "                 │                                 │\n",
    "                 ▼                                 ▼\n",
    "    ┌─────────────────────────┐      ┌─────────────────────────────┐\n",
    "    │  Feature PERSISTS in    │      │  Feature DELETED            │\n",
    "    │  state vector           │      │  (to_delete = true)         │\n",
    "    │                         │      │                             │\n",
    "    │  • In _features_SLAM    │      │  NOT in state vector        │\n",
    "    │  • Part of covariance   │      │                             │\n",
    "    │  • Updated each frame   │      │  Information extracted,     │\n",
    "    │  • Marginalized if lost │      │  then discarded             │\n",
    "    │    or fails χ² test     │      │                             │\n",
    "    └─────────────────────────┘      └─────────────────────────────┘\n",
    "                 │\n",
    "                 │ (Continue updating until...)\n",
    "                 │\n",
    "                 ▼\n",
    "    ┌─────────────────────────────────┐\n",
    "    │  SLAM Feature Marginalization   │\n",
    "    │                                 │\n",
    "    │  Triggered when:                │\n",
    "    │  • Lost tracking                │\n",
    "    │  • update_fail_count > 1        │\n",
    "    │  • Anchor clone marginalized    │\n",
    "    │    (perform anchor change)      │\n",
    "    └─────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c1984",
   "metadata": {},
   "source": [
    "The state vector of our visual-inertial system consists of the current inertial navigation state, a set of c historical IMU\n",
    "pose clones, a set of m environmental landmarks, and a set of w cameras’ extrinsic and intrinsic parameters. The MSCKF features/landmarks never become a part of the state. Only those that successfully gets initialized by SLAM are added to state. (Not all MSCKF features or max tracks features become a part of slam since there are parameters like max_slam_features that bound the slam features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99093e7f",
   "metadata": {},
   "source": [
    "$\\mathbf{x}_k = \\begin{bmatrix} \\mathbf{x}_I^\\mathsf{T} & \\mathbf{x}_C^\\mathsf{T} & \\mathbf{x}_M^\\mathsf{T} & \\mathbf{x}_W^\\mathsf{T} & {}^\\mathcal{C}t_I \\end{bmatrix}^\\mathsf{T}$\n",
    "\n",
    "$\\mathbf{x}_I = \\begin{bmatrix} {}^{I_k}_{\\mkern-2mu G}\\bar{\\mathbf{q}}^\\mathsf{T} & {}^{G}_{\\mkern-2mu}\\mathbf{p}_{I_k}^\\mathsf{T} & {}^{G}_{\\mkern-2mu}\\mathbf{v}_{I_k}^\\mathsf{T} & \\mathbf{b}_{\\omega_k}^\\mathsf{T} & \\mathbf{b}_{a_k}^\\mathsf{T} \\end{bmatrix}^\\mathsf{T}$\n",
    "\n",
    "$\\mathbf{x}_C = \\begin{bmatrix} {}^{I_{k-1}}_{\\mkern-2mu G}\\bar{\\mathbf{q}}^\\mathsf{T} & {}^{G}_{\\mkern-2mu}\\mathbf{p}_{I_{k-1}}^\\mathsf{T} & \\cdots & {}^{I_{k-c}}_{\\mkern-2mu G}\\bar{\\mathbf{q}}^\\mathsf{T} & {}^{G}_{\\mkern-2mu}\\mathbf{p}_{I_{k-c}}^\\mathsf{T} \\end{bmatrix}^\\mathsf{T}$\n",
    "\n",
    "$\\mathbf{x}_M = \\begin{bmatrix} {}^{G}_{\\mkern-2mu}\\mathbf{p}_{f_1}^\\mathsf{T} & \\cdots & {}^{G}_{\\mkern-2mu}\\mathbf{p}_{f_m}^\\mathsf{T} \\end{bmatrix}^\\mathsf{T}$\n",
    "\n",
    "$\\mathbf{x}_W = \\begin{bmatrix} {}^{I}_{\\mkern-2mu C_1}\\bar{\\mathbf{q}}^\\mathsf{T} & {}^{C_1}_{\\mkern-2mu}\\mathbf{p}_I^\\mathsf{T} & \\boldsymbol{\\zeta}_0^\\mathsf{T} & \\cdots & {}^{I}_{\\mkern-2mu C_w}\\bar{\\mathbf{q}}^\\mathsf{T} & {}^{C_w}_{\\mkern-2mu}\\mathbf{p}_I^\\mathsf{T} & \\boldsymbol{\\zeta}_w^\\mathsf{T} \\end{bmatrix}^\\mathsf{T}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e86a786",
   "metadata": {},
   "source": [
    "Nullspace projection only happens when the position and covariance of the feature is unknown. It happens when MSCKF short tracks are marginalized out (MSCKF update) and SLAM long tracks are added to state (delayed init). In case of an slam update when the features are already in the state vector we dont do nullspace projection except for some different represenation which is different in how it stores the state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60cc618",
   "metadata": {},
   "source": [
    "### MSCKF Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d971f5",
   "metadata": {},
   "source": [
    "In case of MSCKF features are not in state. MSCKF doesnt deal with initializing slam features. Initializing slam features follows similarly. The steps are:\n",
    "1. Triangulate features from multiple views\n",
    "2. Compute H_f (Jacobian w.r.t. feature) and H_x (Jacobian w.r.t. state)\n",
    "3. Nullspace projection: Projects out the feature dependency using QR decomposition (see docs)\n",
    "4. Update state with only H_x (feature is eliminated from the equations)\n",
    "5. Delete features immediately after use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee351114",
   "metadata": {},
   "source": [
    "The classic error state Kalman filter uses the following for measurement updates:\n",
    "$$r = H\\tilde{x} + noise$$\n",
    "H is measurement jacobian matrix, noise is zero mean white, uncorrelated to state error.\n",
    "\n",
    "In here the measurement model applies for a single feature, $f_j$, that has been observed from a set of $M_j$ camera poses $({}^{C_i}_{\\mkern-2mu G}{\\bar{\\mathbf{q}}}, {}^{G}{\\mathbf{p}}_{C_i}), i \\in S_j$. Each of the $M_j$ observations of the feature is described by the model:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_i^{(j)} = \\frac{1}{{}^{C_i} Z_j} \\begin{bmatrix} {}^{C_i} X_j \\\\ {}^{C_i} Y_j \\end{bmatrix} + \\mathbf{n}_i^{(j)}, \\quad i \\in S_j\n",
    "$$\n",
    "\n",
    "where $\\mathbf{n}_i^{(j)}$ is the $2 \\times 1$ image noise vector, with covariance matrix $\\mathbf{R}_i^{(j)} = \\sigma_{\\text{im}}^2 \\mathbf{I}_2$. The feature position expressed in the camera frame, ${}^{C_i}{\\mathbf{p}}_{f_j}$, is given by:\n",
    "\n",
    "$$\n",
    "{}^{C_i}{\\mathbf{p}}_{f_j} = \\begin{bmatrix} {}^{C_i} X_j \\\\ {}^{C_i} Y_j \\\\ {}^{C_i} Z_j \\end{bmatrix} = \\mathbf{C}({}^{C_i}_{\\mkern-2mu G}{\\bar{\\mathbf{q}}})({}^{G}{\\mathbf{p}}_{f_j} - {}^{G}{\\mathbf{p}}_{C_i})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00c6db",
   "metadata": {},
   "source": [
    "where ${}^{G}{\\mathbf{p}}_{f_j}$ is the 3D feature position in global frame. Since this is unknown, in the first step of the algorithm triangulation is used to obtain an estimate, ${}^{G}{\\mathbf{p}}_{f_j}$ , of the feature position. This is achieved using the pixel measurements z(j), i ∈ Sj , and the filter estimates of\n",
    "the camera poses at the corresponding time instants (assumed to be known exactly.). Once the estimate of feature is known, its converted to pixel coordinates and residual is computed for measurement update:\n",
    "$$\\mathbf{r}_i^{(j)} = \\mathbf{z}_i^{(j)} - \\hat{\\mathbf{z}}_i^{(j)}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\hat{\\mathbf{z}}_i^{(j)} = \\frac{1}{{}^{C_i} \\hat{Z_j}} \\begin{bmatrix} {}^{C_i} \\hat{X_j} \\\\ {}^{C_i} \\hat{Y_j} \\end{bmatrix}, \\quad \\begin{bmatrix} {}^{C_i} \\hat{X_j} \\\\ {}^{C_i} \\hat{Y_j} \\\\ {}^{C_i} \\hat{Z_j} \\end{bmatrix} = \\mathbf{C}({}^{C_i}_{\\mkern-2mu G}\\hat{\\bar{\\mathbf{q}}})({}^{G}_{\\mkern-2mu}\\hat{\\mathbf{p}}_{f_j} - {}^{G}_{\\mkern-2mu}\\hat{\\mathbf{p}}_{C_i})$$\n",
    "\n",
    "Linearizing about the estimates for the camera pose and for the feature position, the residual of Eq. (20) can be approximated as:\n",
    "\n",
    "$$\\mathbf{r}_i^{(j)} \\approx \\mathbf{H}_{\\mathbf{X_i}}^{(j)} \\tilde{\\mathbf{X}} + \\mathbf{H}_{f_i}^{(j)} {}^{G}_{\\mkern-2mu}\\tilde{\\mathbf{p}}_{f_j} + \\mathbf{n}_i^{(j)}$$\n",
    "\n",
    "In the preceding expression $\\mathbf{H}_{\\mathbf{X_i}}^{(j)}$ and $\\mathbf{H}_{f_i}^{(j)}$ are the Jacobians of the measurement $\\mathbf{z}_i^{(j)}$ with respect to the state and the feature position, respectively, and ${}^{G}_{\\mkern-2mu}\\tilde{\\mathbf{p}}_{f_j}$ is the error in the position estimate of $f_j$. By stacking the residuals of all $M_j$ measurements of this feature, we obtain:\n",
    "\n",
    "$$\\mathbf{r}^{(j)} \\approx \\mathbf{H}_{\\mathbf{X}}^{(j)} \\tilde{\\mathbf{X}} + \\mathbf{H}_{f}^{(j)} {}^{G}_{\\mkern-2mu}\\tilde{\\mathbf{p}}_{f_j} + \\mathbf{n}^{(j)}$$\n",
    "\n",
    "where $\\mathbf{r}^{(j)}$, $\\mathbf{H}_{\\mathbf{X}}^{(j)}$, $\\mathbf{H}_{f}^{(j)}$, and $\\mathbf{n}^{(j)}$ are block vectors or matrices with elements $\\mathbf{r}_i^{(j)}$, $\\mathbf{H}_{\\mathbf{X_i}}^{(j)}$, $\\mathbf{H}_{f_i}^{(j)}$, and $\\mathbf{n}_i^{(j)}$, for $i \\in S_j$. Since the feature observations in different images are independent, the covariance matrix of $\\mathbf{n}^{(j)}$ is $\\mathbf{R}^{(j)} = \\sigma_{\\text{im}}^2 \\mathbf{I}_{2M_j}$.\n",
    "\n",
    "Since state estimate $X$ is used to compute ${}^{G}_{\\mkern-2mu}\\mathbf{p}_{f_j}$, the error ${}^{G}_{\\mkern-2mu}\\tilde{\\mathbf{p}}_{f_j}$ is correlated with the errors in $\\tilde{X}$. Thus the residual is not in $\\tilde{X}$ as in the Kalman filer update equation.\n",
    "\n",
    "This is the reason for null space projection. We define residual $r_o^{(j)}$ by projecting $r^{(j)}$ on the left nullspace of matrix $H_f^{(j)}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea2daeb",
   "metadata": {},
   "source": [
    "To this end for feature j(not mentioned below for clarity), we start with the measurement residual function by removing the \"sensitivity\" to feature error we compute and apply the left nullspace of the Jacobian $\\mathbf{H}_f$. We can compute it using QR decomposition as follows:\n",
    "\n",
    "$$\\mathbf{H}_f = [\\mathbf{Q}_1 \\quad \\mathbf{Q}_2] \\begin{bmatrix} \\mathbf{R}_1 \\\\ \\mathbf{0} \\end{bmatrix} = \\mathbf{Q}_1\\mathbf{R}_1$$\n",
    "\n",
    "Multiplying the linearized measurement equation by the nullspace of the feature Jacobian from the left yields (for each measurement m at time k):\n",
    "\n",
    "$$\\tilde{\\mathbf{r}}_{m,k} \\approx \\mathbf{H}_{\\mathbf{x}}\\tilde{\\mathbf{x}}_k + \\mathbf{Q}_1\\mathbf{R}_1 {}^{G}\\tilde{\\mathbf{p}}_f + \\mathbf{n}_k$$\n",
    "\n",
    "$$\\Rightarrow \\mathbf{Q}_2^\\mathsf{T} \\tilde{\\mathbf{r}}_{m} \\approx \\mathbf{Q}_2^\\mathsf{T} \\mathbf{H}_{\\mathbf{x}}\\tilde{\\mathbf{x}}_k + \\mathbf{Q}_2^\\mathsf{T} \\mathbf{Q}_1\\mathbf{R}_1 {}^{G}\\tilde{\\mathbf{p}}_f + \\mathbf{Q}_2^\\mathsf{T} \\mathbf{n}_k$$\n",
    "\n",
    "$$\\Rightarrow \\mathbf{Q}_2^\\mathsf{T} \\tilde{\\mathbf{r}}_{m} \\approx \\mathbf{Q}_2^\\mathsf{T} \\mathbf{H}_{\\mathbf{x}}\\tilde{\\mathbf{x}}_k + \\mathbf{Q}_2^\\mathsf{T} \\mathbf{n}_k$$\n",
    "\n",
    "$$\\Rightarrow \\tilde{\\mathbf{r}}_{o,k} \\approx \\mathbf{H}_{o,k}\\tilde{\\mathbf{x}}_k + \\mathbf{n}_{o,k}$$\n",
    "\n",
    "$\\mathbf{H}_{o,k}$ can be used to update the EKF.\n",
    "\n",
    "However since a lot of measurements would add a lot of rows in H, the measurements are compressed. \n",
    "\n",
    "#### Measurement Compression\n",
    "\n",
    "One of the most costly operations in the **EKF** update is the matrix multiplication. To mitigate this issue, we perform the thin **QR** decomposition of the measurement **Jacobian** after nullspace projection:\n",
    "\n",
    "$$\n",
    "\\mathbf{H}_{o,k} = [\\mathbf{Q}_1 \\quad \\mathbf{Q}_2] \\begin{bmatrix} \\mathbf{R}_1 \\\\ \\mathbf{0} \\end{bmatrix} = \\mathbf{Q}_1\\mathbf{R}_1\n",
    "$$\n",
    "\n",
    "This **QR** decomposition can be performed again using [Givens rotations](en.wikipedia.org) (note that this operation in general is not cheap though). We apply this **QR** to the linearized measurement residuals to compress measurements:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{r}}_{o,k} \\approx \\mathbf{H}_{o,k}\\tilde{\\mathbf{x}}_k + \\mathbf{n}_o\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{r}}_{o,k} \\approx \\mathbf{Q}_1\\mathbf{R}_1\\tilde{\\mathbf{x}}_k + \\mathbf{n}_o\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{Q}_1^\\mathsf{T} \\tilde{\\mathbf{r}}_{o,k} \\approx \\mathbf{Q}_1^\\mathsf{T} \\mathbf{Q}_1\\mathbf{R}_1\\tilde{\\mathbf{x}}_k + \\mathbf{Q}_1^\\mathsf{T} \\mathbf{n}_o\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{Q}_1^\\mathsf{T} \\tilde{\\mathbf{r}}_{o,k} \\approx \\mathbf{R}_1\\tilde{\\mathbf{x}}_k + \\mathbf{Q}_1^\\mathsf{T} \\mathbf{n}_o\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \\tilde{\\mathbf{r}}_{n,k} \\approx \\mathbf{H}_{n,k}\\tilde{\\mathbf{x}}_k + \\mathbf{n}_{n}\n",
    "$$\n",
    "\n",
    "As a result, the compressed measurement **Jacobian** will be of the size of the state, which will significantly reduce the **EKF** update cost:\n",
    "$$\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1} + \\mathbf{P}_{k|k-1}\\mathbf{H}_{n,k}^\\mathsf{T}(\\mathbf{H}_{n,k}\\mathbf{P}_{k|k-1}\\mathbf{H}_{n,k}^\\mathsf{T} + \\mathbf{R}_n)^{-1}\\tilde{\\mathbf{z}}_{n,k}$$\n",
    "\n",
    "$$\\mathbf{P}_{k|k} = \\mathbf{P}_{k|k-1} - \\mathbf{P}_{k|k-1}\\mathbf{H}_{n,k}^\\mathsf{T}(\\mathbf{H}_{n,k}\\mathbf{P}_{k|k-1}\\mathbf{H}_{n,k}^\\mathsf{T} + \\mathbf{R}_n)^{-1}\\mathbf{H}_{n,k}\\mathbf{P}_{k|k-1}^\\mathsf{T}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88119e4",
   "metadata": {},
   "source": [
    "```\n",
    "void UpdaterMSCKF::update(std::shared_ptr<State> state, std::vector<std::shared_ptr<Feature>> &feature_vec) {\n",
    "\n",
    "  // Return if no features\n",
    "  if (feature_vec.empty())\n",
    "    return;\n",
    "\n",
    "  // Start timing\n",
    "  boost::posix_time::ptime rT0, rT1, rT2, rT3, rT4, rT5;\n",
    "  rT0 = boost::posix_time::microsec_clock::local_time();\n",
    "\n",
    "  // 0. Get all timestamps our clones are at (and thus valid measurement times)\n",
    "  std::vector<double> clonetimes;\n",
    "  for (const auto &clone_imu : state->_clones_IMU) {\n",
    "    clonetimes.emplace_back(clone_imu.first);\n",
    "  }\n",
    "\n",
    "  // 1. Clean all feature measurements and make sure they all have valid clone times\n",
    "  auto it0 = feature_vec.begin();\n",
    "  while (it0 != feature_vec.end()) {\n",
    "\n",
    "    // Clean the feature\n",
    "    (*it0)->clean_old_measurements(clonetimes);\n",
    "\n",
    "    // Count how many measurements\n",
    "    int ct_meas = 0;\n",
    "    for (const auto &pair : (*it0)->timestamps) {\n",
    "      ct_meas += (*it0)->timestamps[pair.first].size();\n",
    "    }\n",
    "\n",
    "    // Remove if we don't have enough\n",
    "    if (ct_meas < 2) {\n",
    "      (*it0)->to_delete = true;\n",
    "      it0 = feature_vec.erase(it0);\n",
    "    } else {\n",
    "      it0++;\n",
    "    }\n",
    "  }\n",
    "  rT1 = boost::posix_time::microsec_clock::local_time();\n",
    "\n",
    "  // 2. Create vector of cloned *CAMERA* poses at each of our clone timesteps\n",
    "  std::unordered_map<size_t, std::unordered_map<double, FeatureInitializer::ClonePose>> clones_cam;\n",
    "  for (const auto &clone_calib : state->_calib_IMUtoCAM) {\n",
    "\n",
    "    // For this camera, create the vector of camera poses\n",
    "    std::unordered_map<double, FeatureInitializer::ClonePose> clones_cami;\n",
    "    for (const auto &clone_imu : state->_clones_IMU) {\n",
    "\n",
    "      // Get current camera pose\n",
    "      Eigen::Matrix<double, 3, 3> R_GtoCi = clone_calib.second->Rot() * clone_imu.second->Rot();\n",
    "      Eigen::Matrix<double, 3, 1> p_CioinG = clone_imu.second->pos() - R_GtoCi.transpose() * clone_calib.second->pos();\n",
    "\n",
    "      // Append to our map\n",
    "      clones_cami.insert({clone_imu.first, FeatureInitializer::ClonePose(R_GtoCi, p_CioinG)});\n",
    "    }\n",
    "\n",
    "    // Append to our map\n",
    "    clones_cam.insert({clone_calib.first, clones_cami});\n",
    "  }\n",
    "\n",
    "  // 3. Try to triangulate all MSCKF or new SLAM features that have measurements\n",
    "  auto it1 = feature_vec.begin();\n",
    "  while (it1 != feature_vec.end()) {\n",
    "\n",
    "    // Triangulate the feature and remove if it fails\n",
    "    bool success_tri = true;\n",
    "    if (initializer_feat->config().triangulate_1d) {\n",
    "      success_tri = initializer_feat->single_triangulation_1d(*it1, clones_cam);\n",
    "    } else {\n",
    "      success_tri = initializer_feat->single_triangulation(*it1, clones_cam);\n",
    "    }\n",
    "\n",
    "    // Gauss-newton refine the feature\n",
    "    bool success_refine = true;\n",
    "    if (initializer_feat->config().refine_features) {\n",
    "      success_refine = initializer_feat->single_gaussnewton(*it1, clones_cam);\n",
    "    }\n",
    "\n",
    "    // Remove the feature if not a success\n",
    "    if (!success_tri || !success_refine) {\n",
    "      (*it1)->to_delete = true;\n",
    "      it1 = feature_vec.erase(it1);\n",
    "      continue;\n",
    "    }\n",
    "    it1++;\n",
    "  }\n",
    "  rT2 = boost::posix_time::microsec_clock::local_time();\n",
    "\n",
    "  // Calculate the max possible measurement size\n",
    "  size_t max_meas_size = 0;\n",
    "  for (size_t i = 0; i < feature_vec.size(); i++) {\n",
    "    for (const auto &pair : feature_vec.at(i)->timestamps) {\n",
    "      max_meas_size += 2 * feature_vec.at(i)->timestamps[pair.first].size();\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Calculate max possible state size (i.e. the size of our covariance)\n",
    "  // NOTE: that when we have the single inverse depth representations, those are only 1dof in size\n",
    "  size_t max_hx_size = state->max_covariance_size();\n",
    "  for (auto &landmark : state->_features_SLAM) {\n",
    "    max_hx_size -= landmark.second->size();\n",
    "  }\n",
    "\n",
    "  // Large Jacobian and residual of *all* features for this update\n",
    "  Eigen::VectorXd res_big = Eigen::VectorXd::Zero(max_meas_size);\n",
    "  Eigen::MatrixXd Hx_big = Eigen::MatrixXd::Zero(max_meas_size, max_hx_size);\n",
    "  std::unordered_map<std::shared_ptr<Type>, size_t> Hx_mapping;\n",
    "  std::vector<std::shared_ptr<Type>> Hx_order_big;\n",
    "  size_t ct_jacob = 0;\n",
    "  size_t ct_meas = 0;\n",
    "\n",
    "  // 4. Compute linear system for each feature, nullspace project, and reject\n",
    "  auto it2 = feature_vec.begin();\n",
    "  while (it2 != feature_vec.end()) {\n",
    "\n",
    "    // Convert our feature into our current format\n",
    "    UpdaterHelper::UpdaterHelperFeature feat;\n",
    "    feat.featid = (*it2)->featid;\n",
    "    feat.uvs = (*it2)->uvs;\n",
    "    feat.uvs_norm = (*it2)->uvs_norm;\n",
    "    feat.timestamps = (*it2)->timestamps;\n",
    "\n",
    "    // If we are using single inverse depth, then it is equivalent to using the msckf inverse depth\n",
    "    feat.feat_representation = state->_options.feat_rep_msckf;\n",
    "    if (state->_options.feat_rep_msckf == LandmarkRepresentation::Representation::ANCHORED_INVERSE_DEPTH_SINGLE) {\n",
    "      feat.feat_representation = LandmarkRepresentation::Representation::ANCHORED_MSCKF_INVERSE_DEPTH;\n",
    "    }\n",
    "\n",
    "    // Save the position and its fej value\n",
    "    if (LandmarkRepresentation::is_relative_representation(feat.feat_representation)) {\n",
    "      feat.anchor_cam_id = (*it2)->anchor_cam_id;\n",
    "      feat.anchor_clone_timestamp = (*it2)->anchor_clone_timestamp;\n",
    "      feat.p_FinA = (*it2)->p_FinA;\n",
    "      feat.p_FinA_fej = (*it2)->p_FinA;\n",
    "    } else {\n",
    "      feat.p_FinG = (*it2)->p_FinG;\n",
    "      feat.p_FinG_fej = (*it2)->p_FinG;\n",
    "    }\n",
    "\n",
    "    // Our return values (feature jacobian, state jacobian, residual, and order of state jacobian)\n",
    "    Eigen::MatrixXd H_f;\n",
    "    Eigen::MatrixXd H_x;\n",
    "    Eigen::VectorXd res;\n",
    "    std::vector<std::shared_ptr<Type>> Hx_order;\n",
    "\n",
    "    // Get the Jacobian for this feature\n",
    "    UpdaterHelper::get_feature_jacobian_full(state, feat, H_f, H_x, res, Hx_order);\n",
    "\n",
    "    // Nullspace project\n",
    "    UpdaterHelper::nullspace_project_inplace(H_f, H_x, res);\n",
    "\n",
    "    /// Chi2 distance check\n",
    "    Eigen::MatrixXd P_marg = StateHelper::get_marginal_covariance(state, Hx_order);\n",
    "    Eigen::MatrixXd S = H_x * P_marg * H_x.transpose();\n",
    "    S.diagonal() += _options.sigma_pix_sq * Eigen::VectorXd::Ones(S.rows());\n",
    "    double chi2 = res.dot(S.llt().solve(res));\n",
    "\n",
    "    // Get our threshold (we precompute up to 500 but handle the case that it is more)\n",
    "    double chi2_check;\n",
    "    if (res.rows() < 500) {\n",
    "      chi2_check = chi_squared_table[res.rows()];\n",
    "    } else {\n",
    "      boost::math::chi_squared chi_squared_dist(res.rows());\n",
    "      chi2_check = boost::math::quantile(chi_squared_dist, 0.95);\n",
    "      PRINT_WARNING(YELLOW \"chi2_check over the residual limit - %d\\n\" RESET, (int)res.rows());\n",
    "    }\n",
    "\n",
    "    // Check if we should delete or not\n",
    "    if (chi2 > _options.chi2_multipler * chi2_check) {\n",
    "      (*it2)->to_delete = true;\n",
    "      it2 = feature_vec.erase(it2);\n",
    "      // PRINT_DEBUG(\"featid = %d\\n\", feat.featid);\n",
    "      // PRINT_DEBUG(\"chi2 = %f > %f\\n\", chi2, _options.chi2_multipler*chi2_check);\n",
    "      // std::stringstream ss;\n",
    "      // ss << \"res = \" << std::endl << res.transpose() << std::endl;\n",
    "      // PRINT_DEBUG(ss.str().c_str());\n",
    "      continue;\n",
    "    }\n",
    "\n",
    "    // We are good!!! Append to our large H vector\n",
    "    size_t ct_hx = 0;\n",
    "    for (const auto &var : Hx_order) {\n",
    "\n",
    "      // Ensure that this variable is in our Jacobian\n",
    "      if (Hx_mapping.find(var) == Hx_mapping.end()) {\n",
    "        Hx_mapping.insert({var, ct_jacob});\n",
    "        Hx_order_big.push_back(var);\n",
    "        ct_jacob += var->size();\n",
    "      }\n",
    "\n",
    "      // Append to our large Jacobian\n",
    "      Hx_big.block(ct_meas, Hx_mapping[var], H_x.rows(), var->size()) = H_x.block(0, ct_hx, H_x.rows(), var->size());\n",
    "      ct_hx += var->size();\n",
    "    }\n",
    "\n",
    "    // Append our residual and move forward\n",
    "    res_big.block(ct_meas, 0, res.rows(), 1) = res;\n",
    "    ct_meas += res.rows();\n",
    "    it2++;\n",
    "  }\n",
    "  rT3 = boost::posix_time::microsec_clock::local_time();\n",
    "\n",
    "  // We have appended all features to our Hx_big, res_big\n",
    "  // Delete it so we do not reuse information\n",
    "  for (size_t f = 0; f < feature_vec.size(); f++) {\n",
    "    feature_vec[f]->to_delete = true;\n",
    "  }\n",
    "\n",
    "  // Return if we don't have anything and resize our matrices\n",
    "  if (ct_meas < 1) {\n",
    "    return;\n",
    "  }\n",
    "  assert(ct_meas <= max_meas_size);\n",
    "  assert(ct_jacob <= max_hx_size);\n",
    "  res_big.conservativeResize(ct_meas, 1);\n",
    "  Hx_big.conservativeResize(ct_meas, ct_jacob);\n",
    "\n",
    "  // 5. Perform measurement compression\n",
    "  UpdaterHelper::measurement_compress_inplace(Hx_big, res_big);\n",
    "  if (Hx_big.rows() < 1) {\n",
    "    return;\n",
    "  }\n",
    "  rT4 = boost::posix_time::microsec_clock::local_time();\n",
    "\n",
    "  // Our noise is isotropic, so make it here after our compression\n",
    "  Eigen::MatrixXd R_big = _options.sigma_pix_sq * Eigen::MatrixXd::Identity(res_big.rows(), res_big.rows());\n",
    "\n",
    "  // 6. With all good features update the state\n",
    "  StateHelper::EKFUpdate(state, Hx_order_big, Hx_big, res_big, R_big);\n",
    "  rT5 = boost::posix_time::microsec_clock::local_time();\n",
    "\n",
    "  // Debug print timing information\n",
    "  PRINT_ALL(\"[MSCKF-UP]: %.4f seconds to clean\\n\", (rT1 - rT0).total_microseconds() * 1e-6);\n",
    "  PRINT_ALL(\"[MSCKF-UP]: %.4f seconds to triangulate\\n\", (rT2 - rT1).total_microseconds() * 1e-6);\n",
    "  PRINT_ALL(\"[MSCKF-UP]: %.4f seconds create system (%d features)\\n\", (rT3 - rT2).total_microseconds() * 1e-6, (int)feature_vec.size());\n",
    "  PRINT_ALL(\"[MSCKF-UP]: %.4f seconds compress system\\n\", (rT4 - rT3).total_microseconds() * 1e-6);\n",
    "  PRINT_ALL(\"[MSCKF-UP]: %.4f seconds update state (%d size)\\n\", (rT5 - rT4).total_microseconds() * 1e-6, (int)res_big.rows());\n",
    "  PRINT_ALL(\"[MSCKF-UP]: %.4f seconds total\\n\", (rT5 - rT1).total_microseconds() * 1e-6);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb27c14",
   "metadata": {},
   "source": [
    "### Slam updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c573f",
   "metadata": {},
   "source": [
    "#### Delayed init\n",
    "- Triangulate\n",
    "- Chi² test        \n",
    "- Initialize to state vector\n",
    "- Expand covariance \n",
    "\n",
    "Delayed init happens when a msckf feature needs to be added to the state vector. This is normally done when the feature has survived max clones number of camera poses. Check feature lifecycle for more information.\n",
    "The procedure for initializing the feature is similar to MSCKF update in that after triangulation gives mean position of the feature in world frame, the null space projection is done. This provides linear equations for error/residual in feature position which can be derived from the non null space projected part. The null space projected part is used to update the EKF. The final feature estimate would be added to the state vector and covariance changed accordingly.\n",
    "\n",
    "Specifically, given a set of measurements involving the state **x** and a new feature **f**, we want to optimally and efficiently initialize the feature.\n",
    "\n",
    "$$z_i = h_i(\\mathbf{x}, \\mathbf{f}) + \\mathbf{n}$$\n",
    "\n",
    "In general, we collect more than the minimum number of measurements at different times needed for initialization (i.e. delayed). For example, although in principle we need two monocular images to initialize a 3D point feature, we often collect more than two images in order to obtain better initialization. In openvins code initialization happens only when a feature is seen in max clones camera poses and is ready to be converted to SLAM point and consequently added to state vector. To process all collected measurements, we stack them and perform linearization around some linearization points (estimates) denoted by $\\mathbf{\\hat{x}}$ and $\\mathbf{\\hat{f}}$:\n",
    "\n",
    "$$\\mathbf{z} = \\begin{bmatrix} \\mathbf{z}_1 \\\\ \\mathbf{z}_2 \\\\ \\vdots \\\\ \\mathbf{z}_m \\end{bmatrix} = \\mathbf{h}(\\mathbf{{x}}, \\mathbf{{f}}) + \\mathbf{n}$$\n",
    "\n",
    "$$\\implies \\mathbf{r} = \\mathbf{z} - \\mathbf{h}(\\mathbf{\\hat{x}}, \\mathbf{\\hat{f}}) = \\mathbf{H}_\\mathbf{x}\\mathbf{\\tilde{x}} + \\mathbf{H}_\\mathbf{f}\\mathbf{\\tilde{f}} + \\mathbf{n}$$\n",
    "\n",
    "To efficiently compute the resulting augmented covariance matrix, we perform **Givens rotations** to zero-out rows in $\\mathbf{H}_\\mathbf{f}$ with indices larger than the dimension of $\\mathbf{f}$, and apply the same Givens rotations to $\\mathbf{H}_\\mathbf{x}$ and $\\mathbf{r}$. As a result of this operation, we have the following linear system:\n",
    "\n",
    "$$\\begin{bmatrix} \\mathbf{r}_1 \\\\ \\mathbf{r}_2 \\end{bmatrix} = \\begin{bmatrix} \\mathbf{H}_{1\\mathbf{x}} \\\\ \\mathbf{H}_{2\\mathbf{x}} \\end{bmatrix}\\mathbf{\\tilde{x}} + \\begin{bmatrix} \\mathbf{H}_{1\\mathbf{f}} \\\\ \\mathbf{0} \\end{bmatrix}\\mathbf{\\tilde{f}} + \\begin{bmatrix} \\mathbf{n}_1 \\\\ \\mathbf{n}_2 \\end{bmatrix}$$\n",
    "\n",
    "Note that the bottom system essentially is corresponding to the nullspace projection as in the MSCKF update and $\\mathbf{H}_{1\\mathbf{f}}$ is generally invertible. Note also that we assume the measurement noise is isotropic; otherwise, we should first perform whitening to make it isotropic, which would save significant computations. So, if the original measurement noise covariance $R = \\sigma^2 I_m$ and the dimension of $\\mathbf{f}$ is $n$, then the inferred measurement noise covariance will be $R_1 = \\sigma^2 I_n$ and $R_2 = \\sigma^2 I_{m-n}$.\n",
    "\n",
    "Now we can directly solve for the error of the new feature based on the first subsystem:\n",
    "\n",
    "$$\\mathbf{\\tilde{f}} = \\mathbf{H}_{1\\mathbf{f}}^{-1}(\\mathbf{r}_1 - \\mathbf{n}_1 - \\mathbf{H}_{1\\mathbf{x}}\\mathbf{\\tilde{x}})$$\n",
    "$$\\implies E[\\mathbf{\\tilde{f}}] = \\mathbf{H}_{1\\mathbf{f}}^{-1}(\\mathbf{r}_1)$$\n",
    "\n",
    "where we assumed noise and state error are zero mean. We can update $\\mathbf{\\hat{f}}$ with this correction by $\\mathbf{\\hat{f}} + E[\\mathbf{\\tilde{f}}]$. \n",
    "\n",
    "We now can compute the covariance of the new feature as follows:\n",
    "\n",
    "$$P_{\\mathbf{f}\\mathbf{f}} = E[(\\mathbf{\\tilde{f}} - E[\\mathbf{\\tilde{f}}])(\\mathbf{\\tilde{f}} - E[\\mathbf{\\tilde{f}}])^T] = E[(\\mathbf{H}_{1\\mathbf{f}}^{-1}(-\\mathbf{n}_1 - \\mathbf{H}_{1\\mathbf{x}}\\mathbf{\\tilde{x}}))(\\mathbf{H}_{1\\mathbf{f}}^{-1}(-\\mathbf{n}_1 - \\mathbf{H}_{1\\mathbf{x}}\\mathbf{\\tilde{x}}))^T]$$\n",
    "$$= \\mathbf{H}_{1\\mathbf{f}}^{-1}(\\mathbf{H}_{1\\mathbf{x}}P_{\\mathbf{x}\\mathbf{x}}\\mathbf{H}_{1\\mathbf{x}}^T + R_1)\\mathbf{H}_{1\\mathbf{f}}^{-T}$$\n",
    "\n",
    "and the cross correlation can be computed as:\n",
    "\n",
    "$$P_{\\mathbf{x}\\mathbf{f}} = E[\\mathbf{\\tilde{x}}(\\mathbf{\\tilde{f}} - E[\\mathbf{\\tilde{f}}])^T] = E[\\mathbf{\\tilde{x}}(\\mathbf{H}_{1\\mathbf{f}}^{-1}(-\\mathbf{n}_1 - \\mathbf{H}_{1\\mathbf{x}}\\mathbf{\\tilde{x}}))^T]$$\n",
    "$$= -P_{\\mathbf{x}\\mathbf{x}}\\mathbf{H}_{1\\mathbf{x}}^T\\mathbf{H}_{1\\mathbf{f}}^{-T}$$\n",
    "\n",
    "These entries can then be placed in the correct location for the covariance. For example when initializing a new feature to the end of the state, the augmented covariance would be:\n",
    "\n",
    "$$P_{\\text{aug}} = \\begin{bmatrix} P_{\\mathbf{x}\\mathbf{x}} & P_{\\mathbf{x}\\mathbf{f}} \\\\ P_{\\mathbf{x}\\mathbf{f}}^T & P_{\\mathbf{f}\\mathbf{f}} \\end{bmatrix}$$\n",
    "After initialization, $r_2, H_{x_2}, n_2$ can be used to update our new state through a standard EKF update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85edc1",
   "metadata": {},
   "source": [
    "#### Implementation details\n",
    "For all the features individually:\n",
    "Create datastructure for features with pFinG from triangulation\n",
    "For null space projection, we need to find out H_f, H_x (jacobians) and res as in r above. res would be [2xtotal_meas, 1], H_f would be [2xtotal_meas, 3], H_X would be [2xtotal_meas, total_hx] where total_hx would be total of calibration, distortion (calibration, distortion would only be added if config asks to add them) and involved clone size.\n",
    "\n",
    "##### Residual computation (res/r)\n",
    "res for a measurement is defined as:\n",
    "$$ res = uv_{m} - uv_{dist} $$\n",
    "$$uv_{dist} = distort(uv_{norm}) $$\n",
    "$$uv_{norm} = p_{FinCi}(0)/p_{FinCi}(2), p_{FinCi}(1)/p_{FinCi}(2) $$\n",
    "where $p_{FinCi}$ is formed for evert measurement \"m\" of each feature:\n",
    "```\n",
    "std::shared_ptr<PoseJPL> clone_Ii = state->_clones_IMU.at(feature.timestamps[pair.first].at(m));\n",
    "Eigen::Matrix3d R_GtoIi = clone_Ii->Rot();\n",
    "Eigen::Vector3d p_IiinG = clone_Ii->pos();\n",
    "\n",
    "// Get current feature in the IMU\n",
    "Eigen::Vector3d p_FinIi = R_GtoIi * (p_FinG - p_IiinG);\n",
    "\n",
    "// Project the current feature into the current frame of reference\n",
    "Eigen::Vector3d p_FinCi = R_ItoC * p_FinIi + p_IinC;\n",
    "```\n",
    "\n",
    "##### Jacobian matrix H_f ($\\partial(r)/\\partial(p_{FinG})$)\n",
    "If fej is to be used, we change the p_FinCi with fej clones rather than updated clones as in:\n",
    "```\n",
    "// If we are doing first estimate Jacobians, then overwrite with the first estimates\n",
    "if (state->_options.do_fej) {\n",
    "R_GtoIi = clone_Ii->Rot_fej();\n",
    "p_IiinG = clone_Ii->pos_fej();\n",
    "// R_ItoC = calibration->Rot_fej();\n",
    "// p_IinC = calibration->pos_fej();\n",
    "p_FinIi = R_GtoIi * (p_FinG_fej - p_IiinG);\n",
    "p_FinCi = R_ItoC * p_FinIi + p_IinC;\n",
    "// uv_norm << p_FinCi(0)/p_FinCi(2),p_FinCi(1)/p_FinCi(2);\n",
    "// cam_d = state->get_intrinsics_CAM(pair.first)->fej();\n",
    "}\n",
    "```\n",
    "\n",
    "The jacobian here are the same as in @dynamic_intialization notebook section Jacobian computation, Fisheye model and perspective projection function. First the computes Jacobians with respect to normalized image coordinates and possibly the camera intrinsics. Then the code proceeds with computing the jacobians for normalized coordinates in respect to projection function dzn_dpfc. Next the derivative of p_FinCi with respect to p_FinIi is computed followed by derivative of p_FinCi in respect to camera clone state. dz_dpfg is computed.\n",
    "\n",
    "Here:\n",
    "dz_dzn is jacobian of uv_dist above wrt uv_norm, dz_dzeta is jacobian of uv_dist wrt intrinsics and distortions, dzn_dpfc is derivative of uv_norm wrt p_FinCi, dpfc_dpfg is p_FinCi wrt p_FinG, dpfc_dclone is p_FinCi wrt clones rotation and position. dz_dpfg is z wrt p_FinG\n",
    "```\n",
    "Eigen::MatrixXd dz_dzn, dz_dzeta;\n",
    "state->_cam_intrinsics_cameras.at(pair.first)->compute_distort_jacobian(uv_norm, dz_dzn, dz_dzeta);\n",
    "\n",
    "// Normalized coordinates in respect to projection function\n",
    "Eigen::MatrixXd dzn_dpfc = Eigen::MatrixXd::Zero(2, 3);\n",
    "dzn_dpfc << 1 / p_FinCi(2), 0, -p_FinCi(0) / (p_FinCi(2) * p_FinCi(2)), 0, 1 / p_FinCi(2), -p_FinCi(1) / (p_FinCi(2) * p_FinCi(2));\n",
    "\n",
    "// Derivative of p_FinCi in respect to p_FinIi\n",
    "Eigen::MatrixXd dpfc_dpfg = R_ItoC * R_GtoIi;\n",
    "\n",
    "// Derivative of p_FinCi in respect to camera clone state\n",
    "Eigen::MatrixXd dpfc_dclone = Eigen::MatrixXd::Zero(3, 6);\n",
    "dpfc_dclone.block(0, 0, 3, 3).noalias() = R_ItoC * skew_x(p_FinIi);\n",
    "dpfc_dclone.block(0, 3, 3, 3) = -dpfc_dpfg;\n",
    "\n",
    "// Precompute some matrices\n",
    "Eigen::MatrixXd dz_dpfc = dz_dzn * dzn_dpfc;\n",
    "Eigen::MatrixXd dz_dpfg = dz_dpfc * dpfc_dpfg;\n",
    "```\n",
    "\n",
    "H_f is build for each c measurement. This portion would be [2x3].\n",
    "```\n",
    "// CHAINRULE: get the total feature Jacobian\n",
    "H_f.block(2 * c, 0, 2, H_f.cols()).noalias() = dz_dpfg * dpfg_dlambda;\n",
    "```\n",
    "\n",
    "H_x is build for each c measurement. Put the jacobian for measurement c at 2*c, clone_Ii/calibration/distortion position which can be found from the map. The size for this portion would be [2x(clone_size/distortion/calibration)].\n",
    "```\n",
    "// CHAINRULE: get state clone Jacobian\n",
    "H_x.block(2 * c, map_hx[clone_Ii], 2, clone_Ii->size()).noalias() = dz_dpfc * dpfc_dclone;\n",
    "\n",
    "// CHAINRULE: loop through all extra states and add their\n",
    "// NOTE: we add the Jacobian here as we might be in the anchoring pose for this measurement\n",
    "for (size_t i = 0; i < dpfg_dx_order.size(); i++) {\n",
    "    H_x.block(2 * c, map_hx[dpfg_dx_order.at(i)], 2, dpfg_dx_order.at(i)->size()).noalias() += dz_dpfg * dpfg_dx.at(i);\n",
    "}\n",
    "\n",
    "//=========================================================================\n",
    "//=========================================================================\n",
    "\n",
    "// Derivative of p_FinCi in respect to camera calibration (R_ItoC, p_IinC)\n",
    "if (state->_options.do_calib_camera_pose) {\n",
    "    // Calculate the Jacobian\n",
    "    Eigen::MatrixXd dpfc_dcalib = Eigen::MatrixXd::Zero(3, 6);\n",
    "    dpfc_dcalib.block(0, 0, 3, 3) = skew_x(p_FinCi - p_IinC);\n",
    "    dpfc_dcalib.block(0, 3, 3, 3) = Eigen::Matrix<double, 3, 3>::Identity();\n",
    "\n",
    "    // Chainrule it and add it to the big jacobian\n",
    "    H_x.block(2 * c, map_hx[calibration], 2, calibration->size()).noalias() += dz_dpfc * dpfc_dcalib;\n",
    "}\n",
    "\n",
    "// Derivative of measurement in respect to distortion parameters\n",
    "if (state->_options.do_calib_camera_intrinsics) {\n",
    "    H_x.block(2 * c, map_hx[distortion], 2, distortion->size()) = dz_dzeta;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6cb80",
   "metadata": {},
   "source": [
    "```\n",
    "void UpdaterSLAM::delayed_init(std::shared_ptr<State> state, std::vector<std::shared_ptr<Feature>> &feature_vec) {\n",
    "\n",
    "  // Return if no features\n",
    "  if (feature_vec.empty())\n",
    "    return;\n",
    "\n",
    "  // Start timing\n",
    "  boost::posix_time::ptime rT0, rT1, rT2, rT3;\n",
    "  rT0 = boost::posix_time::microsec_clock::local_time();\n",
    "\n",
    "  // 0. Get all timestamps our clones are at (and thus valid measurement times)\n",
    "  std::vector<double> clonetimes;\n",
    "  for (const auto &clone_imu : state->_clones_IMU) {\n",
    "    clonetimes.emplace_back(clone_imu.first);\n",
    "  }\n",
    "\n",
    "  // 1. Clean all feature measurements and make sure they all have valid clone times\n",
    "  auto it0 = feature_vec.begin();\n",
    "  while (it0 != feature_vec.end()) {\n",
    "\n",
    "    // Clean the feature\n",
    "    (*it0)->clean_old_measurements(clonetimes);\n",
    "\n",
    "    // Count how many measurements\n",
    "    int ct_meas = 0;\n",
    "    for (const auto &pair : (*it0)->timestamps) {\n",
    "      ct_meas += (*it0)->timestamps[pair.first].size();\n",
    "    }\n",
    "\n",
    "    // Remove if we don't have enough\n",
    "    if (ct_meas < 2) {\n",
    "      (*it0)->to_delete = true;\n",
    "      it0 = feature_vec.erase(it0);\n",
    "    } else {\n",
    "      it0++;\n",
    "    }\n",
    "  }\n",
    "  rT1 = boost::posix_time::microsec_clock::local_time();\n",
    "\n",
    "  // 2. Create vector of cloned *CAMERA* poses at each of our clone timesteps\n",
    "  std::unordered_map<size_t, std::unordered_map<double, FeatureInitializer::ClonePose>> clones_cam;\n",
    "  for (const auto &clone_calib : state->_calib_IMUtoCAM) {\n",
    "\n",
    "    // For this camera, create the vector of camera poses\n",
    "    std::unordered_map<double, FeatureInitializer::ClonePose> clones_cami;\n",
    "    for (const auto &clone_imu : state->_clones_IMU) {\n",
    "\n",
    "      // Get current camera pose\n",
    "      Eigen::Matrix<double, 3, 3> R_GtoCi = clone_calib.second->Rot() * clone_imu.second->Rot();\n",
    "      Eigen::Matrix<double, 3, 1> p_CioinG = clone_imu.second->pos() - R_GtoCi.transpose() * clone_calib.second->pos();\n",
    "\n",
    "      // Append to our map\n",
    "      clones_cami.insert({clone_imu.first, FeatureInitializer::ClonePose(R_GtoCi, p_CioinG)});\n",
    "    }\n",
    "\n",
    "    // Append to our map\n",
    "    clones_cam.insert({clone_calib.first, clones_cami});\n",
    "  }\n",
    "\n",
    "  // 3. Try to triangulate all MSCKF or new SLAM features that have measurements\n",
    "  auto it1 = feature_vec.begin();\n",
    "  while (it1 != feature_vec.end()) {\n",
    "\n",
    "    // Triangulate the feature and remove if it fails\n",
    "    bool success_tri = true;\n",
    "    if (initializer_feat->config().triangulate_1d) {\n",
    "      success_tri = initializer_feat->single_triangulation_1d(*it1, clones_cam);\n",
    "    } else {\n",
    "      success_tri = initializer_feat->single_triangulation(*it1, clones_cam);\n",
    "    }\n",
    "\n",
    "    // Gauss-newton refine the feature\n",
    "    bool success_refine = true;\n",
    "    if (initializer_feat->config().refine_features) {\n",
    "      success_refine = initializer_feat->single_gaussnewton(*it1, clones_cam);\n",
    "    }\n",
    "\n",
    "    // Remove the feature if not a success\n",
    "    if (!success_tri || !success_refine) {\n",
    "      (*it1)->to_delete = true;\n",
    "      it1 = feature_vec.erase(it1);\n",
    "      continue;\n",
    "    }\n",
    "    it1++;\n",
    "  }\n",
    "  rT2 = boost::posix_time::microsec_clock::local_time();\n",
    "\n",
    "  // 4. Compute linear system for each feature, nullspace project, and reject\n",
    "  auto it2 = feature_vec.begin();\n",
    "  while (it2 != feature_vec.end()) {\n",
    "\n",
    "    // Convert our feature into our current format\n",
    "    UpdaterHelper::UpdaterHelperFeature feat;\n",
    "    feat.featid = (*it2)->featid;\n",
    "    feat.uvs = (*it2)->uvs;\n",
    "    feat.uvs_norm = (*it2)->uvs_norm;\n",
    "    feat.timestamps = (*it2)->timestamps;\n",
    "\n",
    "    // If we are using single inverse depth, then it is equivalent to using the msckf inverse depth\n",
    "    auto feat_rep =\n",
    "        ((int)feat.featid < state->_options.max_aruco_features) ? state->_options.feat_rep_aruco : state->_options.feat_rep_slam;\n",
    "    feat.feat_representation = feat_rep;\n",
    "    if (feat_rep == LandmarkRepresentation::Representation::ANCHORED_INVERSE_DEPTH_SINGLE) {\n",
    "      feat.feat_representation = LandmarkRepresentation::Representation::ANCHORED_MSCKF_INVERSE_DEPTH;\n",
    "    }\n",
    "\n",
    "    // Save the position and its fej value\n",
    "    if (LandmarkRepresentation::is_relative_representation(feat.feat_representation)) {\n",
    "      feat.anchor_cam_id = (*it2)->anchor_cam_id;\n",
    "      feat.anchor_clone_timestamp = (*it2)->anchor_clone_timestamp;\n",
    "      feat.p_FinA = (*it2)->p_FinA;\n",
    "      feat.p_FinA_fej = (*it2)->p_FinA;\n",
    "    } else {\n",
    "      feat.p_FinG = (*it2)->p_FinG;\n",
    "      feat.p_FinG_fej = (*it2)->p_FinG;\n",
    "    }\n",
    "\n",
    "    // Our return values (feature jacobian, state jacobian, residual, and order of state jacobian)\n",
    "    Eigen::MatrixXd H_f;\n",
    "    Eigen::MatrixXd H_x;\n",
    "    Eigen::VectorXd res;\n",
    "    std::vector<std::shared_ptr<Type>> Hx_order;\n",
    "\n",
    "    // Get the Jacobian for this feature\n",
    "    UpdaterHelper::get_feature_jacobian_full(state, feat, H_f, H_x, res, Hx_order);\n",
    "\n",
    "    // If we are doing the single feature representation, then we need to remove the bearing portion\n",
    "    // To do so, we project the bearing portion onto the state and depth Jacobians and the residual.\n",
    "    // This allows us to directly initialize the feature as a depth-old feature\n",
    "    if (feat_rep == LandmarkRepresentation::Representation::ANCHORED_INVERSE_DEPTH_SINGLE) {\n",
    "\n",
    "      // Append the Jacobian in respect to the depth of the feature\n",
    "      Eigen::MatrixXd H_xf = H_x;\n",
    "      H_xf.conservativeResize(H_x.rows(), H_x.cols() + 1);\n",
    "      H_xf.block(0, H_x.cols(), H_x.rows(), 1) = H_f.block(0, H_f.cols() - 1, H_f.rows(), 1);\n",
    "      H_f.conservativeResize(H_f.rows(), H_f.cols() - 1);\n",
    "\n",
    "      // Nullspace project the bearing portion\n",
    "      // This takes into account that we have marginalized the bearing already\n",
    "      // Thus this is crucial to ensuring estimator consistency as we are not taking the bearing to be true\n",
    "      UpdaterHelper::nullspace_project_inplace(H_f, H_xf, res);\n",
    "\n",
    "      // Split out the state portion and feature portion\n",
    "      H_x = H_xf.block(0, 0, H_xf.rows(), H_xf.cols() - 1);\n",
    "      H_f = H_xf.block(0, H_xf.cols() - 1, H_xf.rows(), 1);\n",
    "    }\n",
    "\n",
    "    // Create feature pointer (we will always create it of size three since we initialize the single invese depth as a msckf anchored\n",
    "    // representation)\n",
    "    int landmark_size = (feat_rep == LandmarkRepresentation::Representation::ANCHORED_INVERSE_DEPTH_SINGLE) ? 1 : 3;\n",
    "    auto landmark = std::make_shared<Landmark>(landmark_size);\n",
    "    landmark->_featid = feat.featid;\n",
    "    landmark->_feat_representation = feat_rep;\n",
    "    landmark->_unique_camera_id = (*it2)->anchor_cam_id;\n",
    "    if (LandmarkRepresentation::is_relative_representation(feat.feat_representation)) {\n",
    "      landmark->_anchor_cam_id = feat.anchor_cam_id;\n",
    "      landmark->_anchor_clone_timestamp = feat.anchor_clone_timestamp;\n",
    "      landmark->set_from_xyz(feat.p_FinA, false);\n",
    "      landmark->set_from_xyz(feat.p_FinA_fej, true);\n",
    "    } else {\n",
    "      landmark->set_from_xyz(feat.p_FinG, false);\n",
    "      landmark->set_from_xyz(feat.p_FinG_fej, true);\n",
    "    }\n",
    "\n",
    "    // Measurement noise matrix\n",
    "    double sigma_pix_sq =\n",
    "        ((int)feat.featid < state->_options.max_aruco_features) ? _options_aruco.sigma_pix_sq : _options_slam.sigma_pix_sq;\n",
    "    Eigen::MatrixXd R = sigma_pix_sq * Eigen::MatrixXd::Identity(res.rows(), res.rows());\n",
    "\n",
    "    // Try to initialize, delete new pointer if we failed\n",
    "    double chi2_multipler =\n",
    "        ((int)feat.featid < state->_options.max_aruco_features) ? _options_aruco.chi2_multipler : _options_slam.chi2_multipler;\n",
    "    if (StateHelper::initialize(state, landmark, Hx_order, H_x, H_f, R, res, chi2_multipler)) {\n",
    "      state->_features_SLAM.insert({(*it2)->featid, landmark});\n",
    "      (*it2)->to_delete = true;\n",
    "      it2++;\n",
    "    } else {\n",
    "      (*it2)->to_delete = true;\n",
    "      it2 = feature_vec.erase(it2);\n",
    "    }\n",
    "  }\n",
    "  rT3 = boost::posix_time::microsec_clock::local_time();\n",
    "\n",
    "  // Debug print timing information\n",
    "  if (!feature_vec.empty()) {\n",
    "    PRINT_ALL(\"[SLAM-DELAY]: %.4f seconds to clean\\n\", (rT1 - rT0).total_microseconds() * 1e-6);\n",
    "    PRINT_ALL(\"[SLAM-DELAY]: %.4f seconds to triangulate\\n\", (rT2 - rT1).total_microseconds() * 1e-6);\n",
    "    PRINT_ALL(\"[SLAM-DELAY]: %.4f seconds initialize (%d features)\\n\", (rT3 - rT2).total_microseconds() * 1e-6, (int)feature_vec.size());\n",
    "    PRINT_ALL(\"[SLAM-DELAY]: %.4f seconds total\\n\", (rT3 - rT1).total_microseconds() * 1e-6);\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43d7b7",
   "metadata": {},
   "source": [
    "#### Update\n",
    "- Update existing SLAM features  \n",
    "- Chi² test      \n",
    "- H_xf includes feature Jacobian\n",
    "- EKF update "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122ccf4",
   "metadata": {},
   "source": [
    "```\n",
    "void UpdaterSLAM::update(std::shared_ptr<State> state, std::vector<std::shared_ptr<Feature>> &feature_vec) {\n",
    "\n",
    "  // Return if no features\n",
    "  if (feature_vec.empty())\n",
    "    return;\n",
    "\n",
    "  // Start timing\n",
    "  boost::posix_time::ptime rT0, rT1, rT2, rT3;\n",
    "  rT0 = boost::posix_time::microsec_clock::local_time();\n",
    "\n",
    "  // 0. Get all timestamps our clones are at (and thus valid measurement times)\n",
    "  std::vector<double> clonetimes;\n",
    "  for (const auto &clone_imu : state->_clones_IMU) {\n",
    "    clonetimes.emplace_back(clone_imu.first);\n",
    "  }\n",
    "\n",
    "  // 1. Clean all feature measurements and make sure they all have valid clone times\n",
    "  auto it0 = feature_vec.begin();\n",
    "  while (it0 != feature_vec.end()) {\n",
    "\n",
    "    // Clean the feature\n",
    "    (*it0)->clean_old_measurements(clonetimes);\n",
    "\n",
    "    // Count how many measurements\n",
    "    int ct_meas = 0;\n",
    "    for (const auto &pair : (*it0)->timestamps) {\n",
    "      ct_meas += (*it0)->timestamps[pair.first].size();\n",
    "    }\n",
    "\n",
    "    // Get the landmark and its representation\n",
    "    // For single depth representation we need at least two measurement\n",
    "    // This is because we do nullspace projection\n",
    "    std::shared_ptr<Landmark> landmark = state->_features_SLAM.at((*it0)->featid);\n",
    "    int required_meas = (landmark->_feat_representation == LandmarkRepresentation::Representation::ANCHORED_INVERSE_DEPTH_SINGLE) ? 2 : 1;\n",
    "\n",
    "    // Remove if we don't have enough\n",
    "    if (ct_meas < 1) {\n",
    "      (*it0)->to_delete = true;\n",
    "      it0 = feature_vec.erase(it0);\n",
    "    } else if (ct_meas < required_meas) {\n",
    "      it0 = feature_vec.erase(it0);\n",
    "    } else {\n",
    "      it0++;\n",
    "    }\n",
    "  }\n",
    "  rT1 = boost::posix_time::microsec_clock::local_time();\n",
    "\n",
    "  // Calculate the max possible measurement size\n",
    "  size_t max_meas_size = 0;\n",
    "  for (size_t i = 0; i < feature_vec.size(); i++) {\n",
    "    for (const auto &pair : feature_vec.at(i)->timestamps) {\n",
    "      max_meas_size += 2 * feature_vec.at(i)->timestamps[pair.first].size();\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Calculate max possible state size (i.e. the size of our covariance)\n",
    "  size_t max_hx_size = state->max_covariance_size();\n",
    "\n",
    "  // Large Jacobian, residual, and measurement noise of *all* features for this update\n",
    "  Eigen::VectorXd res_big = Eigen::VectorXd::Zero(max_meas_size);\n",
    "  Eigen::MatrixXd Hx_big = Eigen::MatrixXd::Zero(max_meas_size, max_hx_size);\n",
    "  Eigen::MatrixXd R_big = Eigen::MatrixXd::Identity(max_meas_size, max_meas_size);\n",
    "  std::unordered_map<std::shared_ptr<Type>, size_t> Hx_mapping;\n",
    "  std::vector<std::shared_ptr<Type>> Hx_order_big;\n",
    "  size_t ct_jacob = 0;\n",
    "  size_t ct_meas = 0;\n",
    "\n",
    "  // 4. Compute linear system for each feature, nullspace project, and reject\n",
    "  auto it2 = feature_vec.begin();\n",
    "  while (it2 != feature_vec.end()) {\n",
    "\n",
    "    // Ensure we have the landmark and it is the same\n",
    "    assert(state->_features_SLAM.find((*it2)->featid) != state->_features_SLAM.end());\n",
    "    assert(state->_features_SLAM.at((*it2)->featid)->_featid == (*it2)->featid);\n",
    "\n",
    "    // Get our landmark from the state\n",
    "    std::shared_ptr<Landmark> landmark = state->_features_SLAM.at((*it2)->featid);\n",
    "\n",
    "    // Convert the state landmark into our current format\n",
    "    UpdaterHelper::UpdaterHelperFeature feat;\n",
    "    feat.featid = (*it2)->featid;\n",
    "    feat.uvs = (*it2)->uvs;\n",
    "    feat.uvs_norm = (*it2)->uvs_norm;\n",
    "    feat.timestamps = (*it2)->timestamps;\n",
    "\n",
    "    // If we are using single inverse depth, then it is equivalent to using the msckf inverse depth\n",
    "    feat.feat_representation = landmark->_feat_representation;\n",
    "    if (landmark->_feat_representation == LandmarkRepresentation::Representation::ANCHORED_INVERSE_DEPTH_SINGLE) {\n",
    "      feat.feat_representation = LandmarkRepresentation::Representation::ANCHORED_MSCKF_INVERSE_DEPTH;\n",
    "    }\n",
    "\n",
    "    // Save the position and its fej value\n",
    "    if (LandmarkRepresentation::is_relative_representation(feat.feat_representation)) {\n",
    "      feat.anchor_cam_id = landmark->_anchor_cam_id;\n",
    "      feat.anchor_clone_timestamp = landmark->_anchor_clone_timestamp;\n",
    "      feat.p_FinA = landmark->get_xyz(false);\n",
    "      feat.p_FinA_fej = landmark->get_xyz(true);\n",
    "    } else {\n",
    "      feat.p_FinG = landmark->get_xyz(false);\n",
    "      feat.p_FinG_fej = landmark->get_xyz(true);\n",
    "    }\n",
    "\n",
    "    // Our return values (feature jacobian, state jacobian, residual, and order of state jacobian)\n",
    "    Eigen::MatrixXd H_f;\n",
    "    Eigen::MatrixXd H_x;\n",
    "    Eigen::VectorXd res;\n",
    "    std::vector<std::shared_ptr<Type>> Hx_order;\n",
    "\n",
    "    // Get the Jacobian for this feature\n",
    "    UpdaterHelper::get_feature_jacobian_full(state, feat, H_f, H_x, res, Hx_order);\n",
    "\n",
    "    // Place Jacobians in one big Jacobian, since the landmark is already in our state vector\n",
    "    Eigen::MatrixXd H_xf = H_x;\n",
    "    if (landmark->_feat_representation == LandmarkRepresentation::Representation::ANCHORED_INVERSE_DEPTH_SINGLE) {\n",
    "\n",
    "      // Append the Jacobian in respect to the depth of the feature\n",
    "      H_xf.conservativeResize(H_x.rows(), H_x.cols() + 1);\n",
    "      H_xf.block(0, H_x.cols(), H_x.rows(), 1) = H_f.block(0, H_f.cols() - 1, H_f.rows(), 1);\n",
    "      H_f.conservativeResize(H_f.rows(), H_f.cols() - 1);\n",
    "\n",
    "      // Nullspace project the bearing portion\n",
    "      // This takes into account that we have marginalized the bearing already\n",
    "      // Thus this is crucial to ensuring estimator consistency as we are not taking the bearing to be true\n",
    "      UpdaterHelper::nullspace_project_inplace(H_f, H_xf, res);\n",
    "\n",
    "    } else {\n",
    "\n",
    "      // Else we have the full feature in our state, so just append it\n",
    "      H_xf.conservativeResize(H_x.rows(), H_x.cols() + H_f.cols());\n",
    "      H_xf.block(0, H_x.cols(), H_x.rows(), H_f.cols()) = H_f;\n",
    "    }\n",
    "\n",
    "    // Append to our Jacobian order vector\n",
    "    std::vector<std::shared_ptr<Type>> Hxf_order = Hx_order;\n",
    "    Hxf_order.push_back(landmark);\n",
    "\n",
    "    // Chi2 distance check\n",
    "    Eigen::MatrixXd P_marg = StateHelper::get_marginal_covariance(state, Hxf_order);\n",
    "    Eigen::MatrixXd S = H_xf * P_marg * H_xf.transpose();\n",
    "    double sigma_pix_sq =\n",
    "        ((int)feat.featid < state->_options.max_aruco_features) ? _options_aruco.sigma_pix_sq : _options_slam.sigma_pix_sq;\n",
    "    S.diagonal() += sigma_pix_sq * Eigen::VectorXd::Ones(S.rows());\n",
    "    double chi2 = res.dot(S.llt().solve(res));\n",
    "\n",
    "    // Get our threshold (we precompute up to 500 but handle the case that it is more)\n",
    "    double chi2_check;\n",
    "    if (res.rows() < 500) {\n",
    "      chi2_check = chi_squared_table[res.rows()];\n",
    "    } else {\n",
    "      boost::math::chi_squared chi_squared_dist(res.rows());\n",
    "      chi2_check = boost::math::quantile(chi_squared_dist, 0.95);\n",
    "      PRINT_WARNING(YELLOW \"chi2_check over the residual limit - %d\\n\" RESET, (int)res.rows());\n",
    "    }\n",
    "\n",
    "    // Check if we should delete or not\n",
    "    double chi2_multipler =\n",
    "        ((int)feat.featid < state->_options.max_aruco_features) ? _options_aruco.chi2_multipler : _options_slam.chi2_multipler;\n",
    "    if (chi2 > chi2_multipler * chi2_check) {\n",
    "      if ((int)feat.featid < state->_options.max_aruco_features) {\n",
    "        PRINT_WARNING(YELLOW \"[SLAM-UP]: rejecting aruco tag %d for chi2 thresh (%.3f > %.3f)\\n\" RESET, (int)feat.featid, chi2,\n",
    "                      chi2_multipler * chi2_check);\n",
    "      } else {\n",
    "        landmark->update_fail_count++;\n",
    "      }\n",
    "      (*it2)->to_delete = true;\n",
    "      it2 = feature_vec.erase(it2);\n",
    "      continue;\n",
    "    }\n",
    "\n",
    "    // Debug print when we are going to update the aruco tags\n",
    "    if ((int)feat.featid < state->_options.max_aruco_features) {\n",
    "      PRINT_DEBUG(\"[SLAM-UP]: accepted aruco tag %d for chi2 thresh (%.3f < %.3f)\\n\", (int)feat.featid, chi2, chi2_multipler * chi2_check);\n",
    "    }\n",
    "\n",
    "    // We are good!!! Append to our large H vector\n",
    "    size_t ct_hx = 0;\n",
    "    for (const auto &var : Hxf_order) {\n",
    "\n",
    "      // Ensure that this variable is in our Jacobian\n",
    "      if (Hx_mapping.find(var) == Hx_mapping.end()) {\n",
    "        Hx_mapping.insert({var, ct_jacob});\n",
    "        Hx_order_big.push_back(var);\n",
    "        ct_jacob += var->size();\n",
    "      }\n",
    "\n",
    "      // Append to our large Jacobian\n",
    "      Hx_big.block(ct_meas, Hx_mapping[var], H_xf.rows(), var->size()) = H_xf.block(0, ct_hx, H_xf.rows(), var->size());\n",
    "      ct_hx += var->size();\n",
    "    }\n",
    "\n",
    "    // Our isotropic measurement noise\n",
    "    R_big.block(ct_meas, ct_meas, res.rows(), res.rows()) *= sigma_pix_sq;\n",
    "\n",
    "    // Append our residual and move forward\n",
    "    res_big.block(ct_meas, 0, res.rows(), 1) = res;\n",
    "    ct_meas += res.rows();\n",
    "    it2++;\n",
    "  }\n",
    "  rT2 = boost::posix_time::microsec_clock::local_time();\n",
    "\n",
    "  // We have appended all features to our Hx_big, res_big\n",
    "  // Delete it so we do not reuse information\n",
    "  for (size_t f = 0; f < feature_vec.size(); f++) {\n",
    "    feature_vec[f]->to_delete = true;\n",
    "  }\n",
    "\n",
    "  // Return if we don't have anything and resize our matrices\n",
    "  if (ct_meas < 1) {\n",
    "    return;\n",
    "  }\n",
    "  assert(ct_meas <= max_meas_size);\n",
    "  assert(ct_jacob <= max_hx_size);\n",
    "  res_big.conservativeResize(ct_meas, 1);\n",
    "  Hx_big.conservativeResize(ct_meas, ct_jacob);\n",
    "  R_big.conservativeResize(ct_meas, ct_meas);\n",
    "\n",
    "  // 5. With all good SLAM features update the state\n",
    "  StateHelper::EKFUpdate(state, Hx_order_big, Hx_big, res_big, R_big);\n",
    "  rT3 = boost::posix_time::microsec_clock::local_time();\n",
    "\n",
    "  // Debug print timing information\n",
    "  PRINT_ALL(\"[SLAM-UP]: %.4f seconds to clean\\n\", (rT1 - rT0).total_microseconds() * 1e-6);\n",
    "  PRINT_ALL(\"[SLAM-UP]: %.4f seconds creating linear system\\n\", (rT2 - rT1).total_microseconds() * 1e-6);\n",
    "  PRINT_ALL(\"[SLAM-UP]: %.4f seconds to update (%d feats of %d size)\\n\", (rT3 - rT2).total_microseconds() * 1e-6, (int)feature_vec.size(),\n",
    "            (int)Hx_big.rows());\n",
    "  PRINT_ALL(\"[SLAM-UP]: %.4f seconds total\\n\", (rT3 - rT1).total_microseconds() * 1e-6);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10716fd",
   "metadata": {},
   "source": [
    "### Marginalization\n",
    "\n",
    "Marginalization is straightforward. It removes the variables from the state vector and covariance and update the order. I am guessing this is the only thing thats done. For Gaussian distributions, marginalizing out variables simply means removing the corresponding rows and columns from the covariance matrix. The other covariances (P(x₁,x₁), P(x₁,x₂), P(x₂,x₂)) remain unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69be6d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
